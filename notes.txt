This are going to be all the note of the project, including all the concepts required and 
some specific concepts related to the project and the area

Initialy, we have to install with Pip commands the library Scrapy, so, then to start 
correctly our project we need to use the next command to create the enviroment of the project
within our corrent path

$ scrapy startproject ietf_scraper
"scrapy command - sentence - name of the project"

Then to do this, we need to focus on the Spyder's folder, actually, a web scraper is defined 
by many spyders, so, to configure automatically this folder we can follow the next command in cmd
-Remember to stay in the Spyder path

$ scrapy genspider ietf pythonscraping.com
"scrapy command - generate spider folder - name of the folder - the main domain"

this is to connect with the domain of the framework and creates a tamplate of our first spider

This course will use XPATH to create the selectors, so one sentence within the pasre function could be the next

    'number': response.xpath('//span[@class="rfc-no"]/text()').get(), 
    'title': response.xpath('//span[@name="DC.Title"]/@content').get(), 

so, to get the text content of the tag we use "/text()"
to get the value eithin the tag we use "@content"


*********Second Project - WIKIPEDIA
first, i created the new project using "startproject", then in the folder Spider i created the correspond files
using "genspider". 

So, within the Spider/wikipedia.py, first we need to EXTEND SCRAPY'S CLASS 

Before: 
class WikipediaSpider(scrapy.Spider):

After: 
class WikipediaSpider(CrawlSpider):
And mus import this library 
from scrapy.spiders import CrawlSpider

The difference between a scraper and a crawler is that the crawler need to spacify some rules 
We need to give rules for the links to follow, we can do it by using a scrapy rule object





























